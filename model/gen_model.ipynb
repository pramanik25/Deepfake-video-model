{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mface_recognition\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import dlib\n",
    "import face_recognition\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from dataset.loader import normalize_data\n",
    "from decord import VideoReader, cpu\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt # Added for visualization\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- GenConViT Model (same as before) ---\n",
    "\n",
    "def load_genconvit(config, net, ed_weight, vae_weight, fp16):\n",
    "    model = GenConViT(\n",
    "        config,\n",
    "        ed= ed_weight,\n",
    "        vae= vae_weight, \n",
    "        net=net,\n",
    "        fp16=fp16\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    if fp16:\n",
    "        model.half()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def face_rec(frames, p=None, klass=None):\n",
    "    temp_face = np.zeros((len(frames), 224, 224, 3), dtype=np.uint8)\n",
    "    count = 0\n",
    "    mod = \"cnn\" if dlib.DLIB_USE_CUDA else \"hog\"\n",
    "\n",
    "    for _, frame in tqdm(enumerate(frames), total=len(frames)):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        face_locations = face_recognition.face_locations(\n",
    "            frame, number_of_times_to_upsample=0, model=mod\n",
    "        )\n",
    "\n",
    "        for face_location in face_locations:\n",
    "            if count < len(frames):\n",
    "                top, right, bottom, left = face_location\n",
    "                face_image = frame[top:bottom, left:right]\n",
    "                face_image = cv2.resize(\n",
    "                    face_image, (224, 224), interpolation=cv2.INTER_AREA\n",
    "                )\n",
    "                face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                temp_face[count] = face_image\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return ([], 0) if count == 0 else (temp_face[:count], count)\n",
    "\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    df_tensor = torch.tensor(frame, device=device).float()\n",
    "    df_tensor = df_tensor.permute((0, 3, 1, 2))\n",
    "\n",
    "    for i in range(len(df_tensor)):\n",
    "        df_tensor[i] = normalize_data()[\"vid\"](df_tensor[i] / 255.0)\n",
    "\n",
    "    return df_tensor\n",
    "\n",
    "\n",
    "def pred_vid(df, model):\n",
    "    with torch.no_grad():\n",
    "        return max_prediction_value(torch.sigmoid(model(df).squeeze()))\n",
    "\n",
    "\n",
    "def max_prediction_value(y_pred):\n",
    "    # Finds the index and value of the maximum prediction value.\n",
    "    mean_val = torch.mean(y_pred, dim=0)\n",
    "    return (\n",
    "        torch.argmax(mean_val).item(),\n",
    "        mean_val[0].item()\n",
    "        if mean_val[0] > mean_val[1]\n",
    "        else abs(1 - mean_val[1]).item(),\n",
    "    )\n",
    "\n",
    "\n",
    "def real_or_fake(prediction):\n",
    "    return {0: \"REAL\", 1: \"FAKE\"}[prediction ^ 1]\n",
    "\n",
    "\n",
    "def extract_frames(video_file, frames_nums=15):\n",
    "    vr = VideoReader(video_file, ctx=cpu(0))\n",
    "    step_size = max(1, len(vr) // frames_nums)  # Calculate the step size between frames\n",
    "    return vr.get_batch(\n",
    "        list(range(0, len(vr), step_size))[:frames_nums]\n",
    "    ).asnumpy()  # seek frames with step_size\n",
    "\n",
    "\n",
    "def df_face(vid, num_frames, net):\n",
    "    img = extract_frames(vid, num_frames)\n",
    "    face, count = face_rec(img)\n",
    "    return preprocess_frame(face) if count > 0 else []\n",
    "\n",
    "\n",
    "def is_video(vid):\n",
    "    print('IS FILE', os.path.isfile(vid))\n",
    "    return os.path.isfile(vid) and vid.endswith(\n",
    "        tuple([\".avi\", \".mp4\", \".mpg\", \".mpeg\", \".mov\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def set_result():\n",
    "    return {\n",
    "        \"video\": {\n",
    "            \"name\": [],\n",
    "            \"pred\": [],\n",
    "            \"klass\": [],\n",
    "            \"pred_label\": [],\n",
    "            \"correct_label\": [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def store_result(\n",
    "    result, filename, y, y_val, klass, correct_label=None, compression=None\n",
    "):\n",
    "    result[\"video\"][\"name\"].append(filename)\n",
    "    result[\"video\"][\"pred\"].append(y_val)\n",
    "    result[\"video\"][\"klass\"].append(klass.lower())\n",
    "    result[\"video\"][\"pred_label\"].append(real_or_fake(y))\n",
    "\n",
    "    if correct_label is not None:\n",
    "        result[\"video\"][\"correct_label\"].append(correct_label)\n",
    "\n",
    "    if compression is not None:\n",
    "        result[\"video\"][\"compression\"].append(compression)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "class GenConViT(nn.Module):\n",
    "\n",
    "    def __init__(self, config, ed, vae, net, fp16):\n",
    "        super(GenConViT, self).__init__()\n",
    "        self.net = net\n",
    "        self.fp16 = fp16\n",
    "        if self.net=='ed':\n",
    "            try:\n",
    "                from .genconvit_ed import GenConViTED\n",
    "                self.model_ed = GenConViTED(config)\n",
    "                self.checkpoint_ed = torch.load(f'weight/{ed}.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "                if 'state_dict' in self.checkpoint_ed:\n",
    "                    self.model_ed.load_state_dict(self.checkpoint_ed['state_dict'])\n",
    "                else:\n",
    "                    self.model_ed.load_state_dict(self.checkpoint_ed)\n",
    "\n",
    "                self.model_ed.eval()\n",
    "                if self.fp16:\n",
    "                    self.model_ed.half()\n",
    "            except FileNotFoundError:\n",
    "                raise Exception(f\"Error: weight/{ed}.pth file not found.\")\n",
    "        elif self.net=='vae':\n",
    "            try:\n",
    "                from .genconvit_vae import GenConViTVAE\n",
    "                self.model_vae = GenConViTVAE(config)\n",
    "                self.checkpoint_vae = torch.load(f'weight/{vae}.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "                if 'state_dict' in self.checkpoint_vae:\n",
    "                    self.model_vae.load_state_dict(self.checkpoint_vae['state_dict'])\n",
    "                else:\n",
    "                    self.model_vae.load_state_dict(self.checkpoint_vae)\n",
    "                    \n",
    "                self.model_vae.eval()\n",
    "                if self.fp16:\n",
    "                    self.model_vae.half()\n",
    "            except FileNotFoundError:\n",
    "                raise Exception(f\"Error: weight/{vae}.pth file not found.\")\n",
    "        else:\n",
    "            try:\n",
    "                from .genconvit_ed import GenConViTED\n",
    "                from .genconvit_vae import GenConViTVAE\n",
    "                self.model_ed = GenConViTED(config)\n",
    "                self.model_vae = GenConViTVAE(config)\n",
    "                self.checkpoint_ed = torch.load(f'weight/{ed}.pth', map_location=torch.device('cpu'))\n",
    "                self.checkpoint_vae = torch.load(f'weight/{vae}.pth', map_location=torch.device('cpu'))\n",
    "                if 'state_dict' in self.checkpoint_ed:\n",
    "                    self.model_ed.load_state_dict(self.checkpoint_ed['state_dict'])\n",
    "                else:\n",
    "                    self.model_ed.load_state_dict(self.checkpoint_ed)\n",
    "                if 'state_dict' in self.checkpoint_vae:\n",
    "                    self.model_vae.load_state_dict(self.checkpoint_vae['state_dict'])\n",
    "                else:\n",
    "                    self.model_vae.load_state_dict(self.checkpoint_vae)\n",
    "                self.model_ed.eval()\n",
    "                self.model_vae.eval()\n",
    "                if self.fp16:\n",
    "                    self.model_ed.half()\n",
    "                    self.model_vae.half()\n",
    "            except FileNotFoundError as e:\n",
    "                raise Exception(f\"Error: Model weights file not found.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.net == 'ed' :\n",
    "            x = self.model_ed(x)\n",
    "        elif self.net == 'vae':\n",
    "            x,_ = self.model_vae(x)\n",
    "        else:\n",
    "            x1 = self.model_ed(x)\n",
    "            x2,_ = self.model_vae(x)\n",
    "            x =  torch.cat((x1, x2), dim=0) #(x1+x2)/2 #\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Combined Model ---\n",
    "\n",
    "IMG_SIZE = 128\n",
    "FRAMES_PER_VIDEO = 10\n",
    "\n",
    "def create_combined_model(genconvit_config, genconvit_ed_weight, genconvit_vae_weight, genconvit_net,genconvit_fp16 ,input_shape=(FRAMES_PER_VIDEO, 224, 224, 3)):\n",
    "    \n",
    "    \n",
    "    genconvit_model = load_genconvit(genconvit_config, genconvit_net, genconvit_ed_weight, genconvit_vae_weight, genconvit_fp16)\n",
    "    base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False, weights ='imagenet',input_shape=(IMG_SIZE,IMG_SIZE, 3))\n",
    "\n",
    "    base_model.trainable = True\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # GenConViT Feature Extraction \n",
    "    def genconvit_feature_extractor(x):\n",
    "          \n",
    "        x_np = x.numpy()\n",
    "        #print(\"shape of input\",x_np.shape) # (FRAMES_PER_VIDEO,224,224,3)\n",
    "        x_tensor = preprocess_frame(x_np)\n",
    "        with torch.no_grad():\n",
    "            features = genconvit_model(x_tensor)\n",
    "        #print(\"shape of tensor\",features.shape)\n",
    "        return features.cpu().numpy()\n",
    "    \n",
    "    genconvit_layer = tf.keras.layers.Lambda(genconvit_feature_extractor)\n",
    "\n",
    "    genconvit_output = tf.keras.layers.TimeDistributed(genconvit_layer)(inputs) #(None,FRAMES_PER_VIDEO,2,768)\n",
    "\n",
    "    # Reshape tensor \n",
    "    if genconvit_net == 'both':\n",
    "        print(\"The shape in if condition\",genconvit_output.shape)\n",
    "        reshape_layer = tf.keras.layers.Reshape((FRAMES_PER_VIDEO,2,768,1))\n",
    "\n",
    "        reshaped_output = reshape_layer(genconvit_output)\n",
    "    else: \n",
    "         print(\"The shape in else condition\",genconvit_output.shape)\n",
    "         reshape_layer = tf.keras.layers.Reshape((FRAMES_PER_VIDEO,1,768,1))\n",
    "         reshaped_output = reshape_layer(genconvit_output)\n",
    "    # CNN (EfficientNet) for Spatial Feature Extraction\n",
    "    cnn_output= tf.keras.layers.TimeDistributed(base_model)(reshaped_output) # Apply base model to each frame's GenConViT output.\n",
    "\n",
    "    # LSTM for Temporal Modeling\n",
    "    x = tf.keras.layers.GlobalAveragePooling3D()(cnn_output)\n",
    "    x = tf.keras.layers.Reshape((1,-1))(x)\n",
    "    x = tf.keras.layers.LSTM(256)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    combined_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return combined_model\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "def load_and_preprocess_videos(path, label):\n",
    "    videos = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n",
    "            try:\n",
    "                videcap = cv2.VideoCapture(os.path.join(path, filename))\n",
    "                frames = []\n",
    "                success, image = videcap.read()\n",
    "                count = 0\n",
    "                while success and count < FRAMES_PER_VIDEO:\n",
    "                    image = cv2.resize(image, (224, 224))\n",
    "                    frames.append(image)\n",
    "                    success, image = videcap.read()\n",
    "                    count += 1\n",
    "                videcap.release()\n",
    "\n",
    "                if len(frames) == FRAMES_PER_VIDEO:\n",
    "                    videos.append(np.array(frames))\n",
    "                    labels.append(label)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {filename}: {type(e).__name__} - {e}\")\n",
    "\n",
    "    return np.array(videos), np.array(labels)\n",
    "\n",
    "# --- Training and Evaluation ---\n",
    "\n",
    "# Main execution block (for notebook)\n",
    "if __name__ == '__main__':\n",
    "    # Define paths and model parameters\n",
    "    real_videos_path = r\"C:\\Users\\ASUS\\OneDrive\\Attachments\\All Datasets\\Deepfake datset\\Deepfake Celeb DF real\"\n",
    "    fake_videos_path = r\"C:\\Users\\ASUS\\OneDrive\\Attachments\\All Datasets\\Deepfake datset\\Deepfake fake data\"\n",
    "\n",
    "    genconvit_config = {\n",
    "        'image_size': 224,\n",
    "        'patch_size': 16,\n",
    "        'num_classes': 2,\n",
    "        'dim': 768,\n",
    "        'depth': 12,\n",
    "        'heads': 12,\n",
    "        'mlp_dim': 3072,\n",
    "        'dropout': 0.1,\n",
    "        'emb_dropout': 0.1,\n",
    "    }\n",
    "\n",
    "    genconvit_ed_weight = 'ed_best'\n",
    "    genconvit_vae_weight = 'vae_best'\n",
    "    genconvit_net = 'both' # 'ed', 'vae', 'both'\n",
    "    genconvit_fp16 = False\n",
    "\n",
    "    # Load and preprocess data\n",
    "    real_videos, real_labels = load_and_preprocess_videos(real_videos_path, 0)\n",
    "    fake_videos, fake_labels = load_and_preprocess_videos(fake_videos_path, 1)\n",
    "\n",
    "    # Combine real and fake data\n",
    "    X = np.concatenate([real_videos, fake_videos])\n",
    "    Y = np.concatenate([real_labels, fake_labels])\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create the combined model\n",
    "    combined_model = create_combined_model(genconvit_config, genconvit_ed_weight, genconvit_vae_weight, genconvit_net, genconvit_fp16 )\n",
    "\n",
    "    # Compile and train the model\n",
    "    combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = combined_model.fit(\n",
    "        X_train, Y_train,\n",
    "        epochs=2, batch_size=8,  # Adjust based on your resources\n",
    "        validation_data=(X_test, Y_test)\n",
    "    )\n",
    "\n",
    "    # Evaluation and Visualization\n",
    "    _, accuracy = combined_model.evaluate(X_test, Y_test)\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "    # Plotting the training history\n",
    "    def plot_history(history):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    plot_history(history)\n",
    "\n",
    "\n",
    "    # Prediction on new video\n",
    "    def predict_video(video_path):\n",
    "        try:\n",
    "            vidcap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            success, image = vidcap.read()\n",
    "            count = 0\n",
    "            while success and count < FRAMES_PER_VIDEO:\n",
    "                image = cv2.resize(image, (224, 224))\n",
    "                frames.append(image)\n",
    "                success, image = vidcap.read()\n",
    "                count += 1\n",
    "            vidcap.release()\n",
    "            if len(frames) == FRAMES_PER_VIDEO:\n",
    "                video = np.expand_dims(np.array(frames), axis=0)\n",
    "                prediction = combined_model.predict(video)[0][0]\n",
    "                return prediction\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video: {e}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    # Example Prediction\n",
    "    prediction = predict_video(r\"C:\\Users\\ASUS\\OneDrive\\Attachments\\All Datasets\\Deepfake datset\\Deepfake fake data\\id0_id1_0001.mp4\") # replace with test video path\n",
    "    if prediction is not None:\n",
    "        if prediction > 0.5:\n",
    "            print(\"Prediction: Fake (Probability:\", prediction, \")\")\n",
    "        else:\n",
    "            print(\"Prediction: Real (Probability:\", prediction, \")\")\n",
    "    else:\n",
    "      print(\"Error processing video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
