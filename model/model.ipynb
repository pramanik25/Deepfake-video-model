{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_videos_path = r\"C:\\Users\\ASUS\\OneDrive\\Attachments\\All Datasets\\Deepfake datset\\Deepfake Celeb DF real\"\n",
    "fake_videos_path = r\"C:\\Users\\ASUS\\OneDrive\\Attachments\\All Datasets\\Deepfake datset\\Deepfake fake data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "FRAMES_PER_VIDEO = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Extracting the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_videos(path, label):\n",
    "    videos = []\n",
    "    labels  = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".mp4\") or filename.endwith(\".avi\"):\n",
    "            try:\n",
    "                videcap = cv2.VideoCapture(os.path.join(path,filename)) #THis will open the video file\n",
    "               \n",
    "                frames =[]\n",
    "                success, Image = videcap.read()\n",
    "                count = 0\n",
    "                while success and count < FRAMES_PER_VIDEO:   #Extract the fixed num of frames \n",
    "                    Image = cv2.resize(Image, (IMG_SIZE, IMG_SIZE)) #resizing the frames \n",
    "                    frames.append(Image)\n",
    "                    success, Image = videcap.read() #read the next frame\n",
    "\n",
    "                    count+=1\n",
    "                    \n",
    "                videcap.release()\n",
    "\n",
    "                if len(frames) == FRAMES_PER_VIDEO :\n",
    "                      videos.append(np.array(frames))\n",
    "                      labels.append(label)  #Add 0 for real and 1 for fake \n",
    "\n",
    "            except Exception as e:\n",
    "                            print(f\"Error processing video {filename} :{type(e).__name__} - {e}\")\n",
    "                            \n",
    "    return np.array(videos), np.array(labels)\n",
    "\n",
    "              \n",
    "\n",
    "\n",
    "#LOad and preprocess real and fake videos \n",
    "real_videos, real_labels = load_and_preprocess_videos(real_videos_path, 0)\n",
    "\n",
    "fake_videos, fake_labels = load_and_preprocess_videos(fake_videos_path, 1)\n",
    "\n",
    "#combining real and fake data \n",
    "X = np.concatenate([real_videos, fake_videos])\n",
    "Y= np.concatenate([real_labels, fake_labels])\n",
    "\n",
    "#split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.2, random_state=42)\n",
    "                    \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape=(FRAMES_PER_VIDEO, IMG_SIZE, IMG_SIZE, 3)):\n",
    "     \n",
    "     #EfficientNet3D for spatial feature extraction \n",
    "\n",
    "     base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False, weights ='imagenet',input_shape=(IMG_SIZE,IMG_SIZE, 3))\n",
    "\n",
    "     #Load a pre-trained weights (transfer learning)\n",
    "     base_model.trainable = True\n",
    "\n",
    "     inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "     X = tf.keras.layers.TimeDistributed(base_model)(inputs) # Applying EfficientNet to each frame\n",
    "\n",
    "     X = tf.keras.layers.GlobalAveragePooling3D()(X) \n",
    "     X = tf.keras.layers.Reshape((1, -1))(X)\n",
    "\n",
    "\n",
    "     #LSTM for temporal feature extraction \n",
    "     X = tf.keras.layers.LSTM(256)(X)\n",
    "     X = tf.keras.layers.Dense(256, activation='relu')(X)\n",
    "     X = tf.keras.layers.Dropout(0.5)(X)\n",
    "     outputs = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "     model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "     return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model compilation and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2788s\u001b[0m 4s/step - accuracy: 0.8950 - loss: 0.3719 - val_accuracy: 0.8860 - val_loss: 0.3637\n",
      "Epoch 2/2\n",
      "\u001b[1m623/623\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2920s\u001b[0m 5s/step - accuracy: 0.9086 - loss: 0.3181 - val_accuracy: 0.8860 - val_loss: 0.3629\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=2, batch_size=8,  # Adjust based on your resources\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "#ploting the history as loss and accuracy data \n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on model as video is real or fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 24s/step\n",
      "Prediction: Fake (Probability: 0.89594537 )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_video(video_path):\n",
    "    \n",
    "    try:\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        success, image = vidcap.read()\n",
    "        count = 0\n",
    "        while success and count < FRAMES_PER_VIDEO:\n",
    "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "            frames.append(image)\n",
    "            success, image = vidcap.read()\n",
    "            count += 1\n",
    "        vidcap.release()\n",
    "        if len(frames) == FRAMES_PER_VIDEO: \n",
    "            video = np.expand_dims(np.array(frames), axis=0) \n",
    "            prediction = model.predict(video)[0][0]\n",
    "            return prediction\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "prediction = predict_video(r\"C:\\Users\\ASUS\\OneDrive\\Attachments\\All Datasets\\Deepfake datset\\Deepfake fake data\\id0_id1_0001.mp4\")  # Replace with video path\n",
    "\n",
    "if prediction is not None:\n",
    "    if prediction > 0.5:\n",
    "        print(\"Prediction: Fake (Probability:\", prediction, \")\")\n",
    "    else:\n",
    "        print(\"Prediction: Real (Probability:\", prediction, \")\")\n",
    "else:\n",
    "    print(\"Error processing video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
