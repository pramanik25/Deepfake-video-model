{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import tensorflow as tf \n",
    " \n",
    "real_videos_path = r\"C:\\Users\\rajku\\Downloads\\archive\\Celeb-real\"\n",
    "fake_videos_path = r\"C:\\Users\\rajku\\Downloads\\archive\\New Folder\"\n",
    "IMG_SIZE = 128\n",
    "FRAMES_PER_VIDEO = 30\n",
    "#Loading and Extracting the features\n",
    "\n",
    "def load_and_preprocess_videos(path, label):\n",
    "    videos = []\n",
    "    labels  = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".mp4\") or filename.endwith(\".avi\"):\n",
    "            try:\n",
    "                videcap = cv2.VideoCapture(os.path.join(path,filename)) #THis will open the video file\n",
    "               \n",
    "                frames =[]\n",
    "                success, Image = videcap.read()\n",
    "                count = 0\n",
    "                while success and count < FRAMES_PER_VIDEO:   #Extract the fixed num of frames \n",
    "                    Image = cv2.resize(Image, (IMG_SIZE, IMG_SIZE)) #resizing the frames \n",
    "                    frames.append(Image)\n",
    "                    success, Image = videcap.read() #read the next frame\n",
    "\n",
    "                    count+=1\n",
    "                    \n",
    "                videcap.release()\n",
    "\n",
    "                if len(frames) == FRAMES_PER_VIDEO :\n",
    "                      videos.append(np.array(frames))\n",
    "                      labels.append(label)  #Add 0 for real and 1 for fake \n",
    "\n",
    "            except Exception as e:\n",
    "                            print(f\"Error processing video {filename} :{type(e).__name__} - {e}\")\n",
    "                            \n",
    "    return np.array(videos), np.array(labels)\n",
    "\n",
    "              \n",
    "\n",
    "\n",
    "#LOad and preprocess real and fake videos \n",
    "real_videos, real_labels = load_and_preprocess_videos(real_videos_path, 0)\n",
    "\n",
    "fake_videos, fake_labels = load_and_preprocess_videos(fake_videos_path, 1)\n",
    "\n",
    "#combining real and fake data \n",
    "X = np.concatenate([real_videos, fake_videos])\n",
    "Y= np.concatenate([real_labels, fake_labels])\n",
    "\n",
    "#split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.2, random_state=42)\n",
    "                    \n",
    "            \n",
    "\n",
    "def create_model(input_shape=(FRAMES_PER_VIDEO, IMG_SIZE, IMG_SIZE, 3)):\n",
    "     \n",
    "     #EfficientNet3D for spatial feature extraction \n",
    "\n",
    "     base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False, weights ='imagenet',input_shape=(IMG_SIZE,IMG_SIZE, 3))\n",
    "\n",
    "     #Load a pre-trained weights (transfer learning)\n",
    "     base_model.trainable = True\n",
    "\n",
    "     inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "     X = tf.keras.layers.TimeDistributed(base_model)(inputs) # Applying EfficientNet to each frame\n",
    "\n",
    "     X = tf.keras.layers.GlobalAveragePooling3D()(X) \n",
    "     X = tf.keras.layers.Reshape((1, -1))(X)\n",
    "\n",
    "\n",
    "     #LSTM for temporal feature extraction \n",
    "     X = tf.keras.layers.LSTM(256)(X)\n",
    "     X = tf.keras.layers.Dense(256, activation='relu')(X)\n",
    "     X = tf.keras.layers.Dropout(0.5)(X)\n",
    "     outputs = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "     model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "     return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "    \n",
    "#Model compilation and Training\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=2, batch_size=8,  # Adjust based on your resources\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "\n",
    "#Evaluation and Visualization\n",
    "\n",
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "#ploting the history as loss and accuracy data\n",
    "def plot_history(history): plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy') \n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch') \n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plot_history(history)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prediction: Fake (Probability: 0.81851685 )\n"
     ]
    }
   ],
   "source": [
    "#Prediction on model as video is real or fake\n",
    "\n",
    "def predict_video(video_path):\n",
    "    \n",
    "    try:\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        success, image = vidcap.read()\n",
    "        count = 0\n",
    "        while success and count < FRAMES_PER_VIDEO:\n",
    "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "            frames.append(image)\n",
    "            success, image = vidcap.read()\n",
    "            count += 1\n",
    "        vidcap.release()\n",
    "        if len(frames) == FRAMES_PER_VIDEO: \n",
    "            video = np.expand_dims(np.array(frames), axis=0) \n",
    "            prediction = model.predict(video)[0][0]\n",
    "            return prediction\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {e}\")\n",
    "        return None\n",
    "\n",
    "#prediction = predict_video(r\"C:\\Users\\rajku\\Downloads\\archive\\Celeb-synthesis\\id0_id1_0001.mp4\")  # Replace with video path\n",
    "#prediction = predict_video(r\"C:\\Users\\rajku\\Downloads\\archive\\YouTube-real\\00000.mp4\")  # Replace with video path\n",
    "prediction = predict_video(r\"C:\\Users\\rajku\\Downloads\\archive\\Celeb-synthesis\\id61_id60_0009.mp4\")\n",
    "if prediction is not None:\n",
    "    if prediction > 0.5:\n",
    "        print(\"Prediction: Fake (Probability:\", prediction, \")\")\n",
    "    else:\n",
    "        print(\"Prediction: Real (Probability:\", 1-prediction, \")\")\n",
    "else:\n",
    "    print(\"Error processing video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing video: name 'cv2' is not defined\n",
      "Error processing video.\n"
     ]
    }
   ],
   "source": [
    "def predict_video(video_path):\n",
    "    \n",
    "    try:\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        success, image = vidcap.read()\n",
    "        count = 0\n",
    "        while success and count < FRAMES_PER_VIDEO:\n",
    "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "            frames.append(image)\n",
    "            success, image = vidcap.read()\n",
    "            count += 1\n",
    "        vidcap.release()\n",
    "        if len(frames) == FRAMES_PER_VIDEO: \n",
    "            video = np.expand_dims(np.array(frames), axis=0) \n",
    "            prediction = model.predict(video)[0][0]\n",
    "            return prediction\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {e}\")\n",
    "        return None\n",
    "prediction = predict_video(r\"C:\\Users\\rajku\\Downloads\\archive\\Celeb-synthesis\\id16_id31_0007.mp4\")  # Replace with video path\n",
    "#prediction = predict_video(r\"C:\\Users\\ASUS\\OneDrive\\Attachments\\All Datasets\\Deepfake datset\\Deepfake fake data\\id0_id1_0001.mp4\")  # Replace with video path\n",
    "#prediction = predict_video(r\"C:\\Users\\rajku\\Downloads\\y2mate.com - JAWAN Chaleya Hindi  Shah Rukh Khan  Nayanthara  Atlee  Anirudh  Arijit S Shilpa R  Kumaar_1080p.mp4\")\n",
    "if prediction is not None:\n",
    "    if prediction > 0.5:\n",
    "        print(\"Prediction: Fake (Probability:\", prediction, \")\")\n",
    "    else:\n",
    "        print(\"Prediction: Real (Probability:\", 1-prediction, \")\")\n",
    "else:\n",
    "    print(\"Error processing video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajku\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 345\u001b[0m\n\u001b[0;32m    342\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# Create the combined model\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m combined_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_combined_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenconvit_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenconvit_ed_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenconvit_vae_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenconvit_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenconvit_fp16\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Compile and train the model\u001b[39;00m\n\u001b[0;32m    348\u001b[0m combined_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[1], line 233\u001b[0m, in \u001b[0;36mcreate_combined_model\u001b[1;34m(genconvit_config, genconvit_ed_weight, genconvit_vae_weight, genconvit_net, genconvit_fp16, input_shape)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_combined_model\u001b[39m(genconvit_config, genconvit_ed_weight, genconvit_vae_weight, genconvit_net,genconvit_fp16 ,input_shape\u001b[38;5;241m=\u001b[39m(FRAMES_PER_VIDEO, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m)):\n\u001b[1;32m--> 233\u001b[0m     genconvit_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_genconvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenconvit_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenconvit_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenconvit_ed_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenconvit_vae_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenconvit_fp16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m     base_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mapplications\u001b[38;5;241m.\u001b[39mefficientnet_v2\u001b[38;5;241m.\u001b[39mEfficientNetV2B0(include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weights \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m,input_shape\u001b[38;5;241m=\u001b[39m(IMG_SIZE,IMG_SIZE, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m    236\u001b[0m     base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mload_genconvit\u001b[1;34m(config, net, ed_weight, vae_weight, fp16)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_genconvit\u001b[39m(config, net, ed_weight, vae_weight, fp16):\n\u001b[1;32m---> 25\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mGenConViT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43med\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43med_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvae_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[1], line 192\u001b[0m, in \u001b[0;36mGenConViT.__init__\u001b[1;34m(self, config, ed, vae, net, fp16)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenconvit_ed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenConViTED\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenconvit_vae\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenConViTVAE\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_ed \u001b[38;5;241m=\u001b[39m \u001b[43mGenConViTED\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_vae \u001b[38;5;241m=\u001b[39m GenConViTVAE(config)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_ed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00med\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\rajku\\OneDrive\\Desktop\\python folder\\model\\genconvit_ed.py:72\u001b[0m, in \u001b[0;36mGenConViTED.__init__\u001b[1;34m(self, config, pretrained)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m Decoder()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m#self.backbone = timm.create_model(config['model']['backbone'], pretrained=pretrained)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackbone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedder\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}), pretrained\u001b[38;5;241m=\u001b[39mpretrained)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mpatch_embed \u001b[38;5;241m=\u001b[39m HybridEmbed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder, img_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m'\u001b[39m], embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\timm\\models\\_factory.py:103\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, cache_dir, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Parameters that aren't supported by all models or are intended to only override model defaults if set\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# should default to None in command line args/cfg. Remove them if they are present and not set so that\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# non-supporting models don't break and default args remain in effect.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m--> 103\u001b[0m model_source, model_name \u001b[38;5;241m=\u001b[39m \u001b[43mparse_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf-hub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pretrained_cfg, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_cfg should not be set when sourcing model from Hugging Face Hub.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\timm\\models\\_factory.py:17\u001b[0m, in \u001b[0;36mparse_model_name\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_model_name\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_hub\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# NOTE for backwards compat, deprecate hf_hub use\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         model_name \u001b[38;5;241m=\u001b[39m model_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_hub\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf-hub\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m urlsplit(model_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import dlib\n",
    "import face_recognition\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "#from dataset.loader import normalize_data\n",
    "from loader import normalize_data\n",
    "from decord import VideoReader, cpu\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt # Added for visualization\n",
    "#import timm #added from myside\n",
    "#from timm import create_model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- GenConViT Model (same as before) ---\n",
    "\n",
    "def load_genconvit(config, net, ed_weight, vae_weight, fp16):\n",
    "    model = GenConViT(\n",
    "        config,\n",
    "        ed= ed_weight,\n",
    "        vae= vae_weight, \n",
    "        net=net,\n",
    "        fp16=fp16\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    if fp16:\n",
    "        model.half()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def face_rec(frames, p=None, klass=None):\n",
    "    temp_face = np.zeros((len(frames), 224, 224, 3), dtype=np.uint8)\n",
    "    count = 0\n",
    "    mod = \"cnn\" if dlib.DLIB_USE_CUDA else \"hog\"\n",
    "\n",
    "    for _, frame in tqdm(enumerate(frames), total=len(frames)):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        face_locations = face_recognition.face_locations(\n",
    "            frame, number_of_times_to_upsample=0, model=mod\n",
    "        )\n",
    "\n",
    "        for face_location in face_locations:\n",
    "            if count < len(frames):\n",
    "                top, right, bottom, left = face_location\n",
    "                face_image = frame[top:bottom, left:right]\n",
    "                face_image = cv2.resize(\n",
    "                    face_image, (224, 224), interpolation=cv2.INTER_AREA\n",
    "                )\n",
    "                face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                temp_face[count] = face_image\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return ([], 0) if count == 0 else (temp_face[:count], count)\n",
    "\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    df_tensor = torch.tensor(frame, device=device).float()\n",
    "    df_tensor = df_tensor.permute((0, 3, 1, 2))\n",
    "\n",
    "    for i in range(len(df_tensor)):\n",
    "        df_tensor[i] = normalize_data()[\"vid\"](df_tensor[i] / 255.0)\n",
    "\n",
    "    return df_tensor\n",
    "\n",
    "\n",
    "def pred_vid(df, model):\n",
    "    with torch.no_grad():\n",
    "        return max_prediction_value(torch.sigmoid(model(df).squeeze()))\n",
    "\n",
    "\n",
    "def max_prediction_value(y_pred):\n",
    "    # Finds the index and value of the maximum prediction value.\n",
    "    mean_val = torch.mean(y_pred, dim=0)\n",
    "    return (\n",
    "        torch.argmax(mean_val).item(),\n",
    "        mean_val[0].item()\n",
    "        if mean_val[0] > mean_val[1]\n",
    "        else abs(1 - mean_val[1]).item(),\n",
    "    )\n",
    "\n",
    "\n",
    "def real_or_fake(prediction):\n",
    "    return {0: \"REAL\", 1: \"FAKE\"}[prediction ^ 1]\n",
    "\n",
    "\n",
    "def extract_frames(video_file, frames_nums=15):\n",
    "    vr = VideoReader(video_file, ctx=cpu(0))\n",
    "    step_size = max(1, len(vr) // frames_nums)  # Calculate the step size between frames\n",
    "    return vr.get_batch(\n",
    "        list(range(0, len(vr), step_size))[:frames_nums]\n",
    "    ).asnumpy()  # seek frames with step_size\n",
    "\n",
    "\n",
    "def df_face(vid, num_frames, net):\n",
    "    img = extract_frames(vid, num_frames)\n",
    "    face, count = face_rec(img)\n",
    "    return preprocess_frame(face) if count > 0 else []\n",
    "\n",
    "\n",
    "def is_video(vid):\n",
    "    print('IS FILE', os.path.isfile(vid))\n",
    "    return os.path.isfile(vid) and vid.endswith(\n",
    "        tuple([\".avi\", \".mp4\", \".mpg\", \".mpeg\", \".mov\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def set_result():\n",
    "    return {\n",
    "        \"video\": {\n",
    "            \"name\": [],\n",
    "            \"pred\": [],\n",
    "            \"klass\": [],\n",
    "            \"pred_label\": [],\n",
    "            \"correct_label\": [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def store_result(\n",
    "    result, filename, y, y_val, klass, correct_label=None, compression=None\n",
    "):\n",
    "    result[\"video\"][\"name\"].append(filename)\n",
    "    result[\"video\"][\"pred\"].append(y_val)\n",
    "    result[\"video\"][\"klass\"].append(klass.lower())\n",
    "    result[\"video\"][\"pred_label\"].append(real_or_fake(y))\n",
    "\n",
    "    if correct_label is not None:\n",
    "        result[\"video\"][\"correct_label\"].append(correct_label)\n",
    "\n",
    "    if compression is not None:\n",
    "        result[\"video\"][\"compression\"].append(compression)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "class GenConViT(nn.Module):\n",
    "\n",
    "    def __init__(self, config, ed, vae, net, fp16):\n",
    "        super(GenConViT, self).__init__()\n",
    "        self.net = net\n",
    "        self.fp16 = fp16\n",
    "        if self.net=='ed':\n",
    "            try:\n",
    "                from genconvit_ed import GenConViTED\n",
    "                self.model_ed = GenConViTED(config)\n",
    "                self.checkpoint_ed = torch.load(f'weight/{ed}.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "                if 'state_dict' in self.checkpoint_ed:\n",
    "                    self.model_ed.load_state_dict(self.checkpoint_ed['state_dict'])\n",
    "                else:\n",
    "                    self.model_ed.load_state_dict(self.checkpoint_ed)\n",
    "\n",
    "                self.model_ed.eval()\n",
    "                if self.fp16:\n",
    "                    self.model_ed.half()\n",
    "            except FileNotFoundError:\n",
    "                raise Exception(f\"Error: weight/{ed}.pth file not found.\")\n",
    "        elif self.net=='vae':\n",
    "            try:\n",
    "                from genconvit_vae import GenConViTVAE\n",
    "                self.model_vae = GenConViTVAE(config)\n",
    "                self.checkpoint_vae = torch.load(f'weight/{vae}.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "                if 'state_dict' in self.checkpoint_vae:\n",
    "                    self.model_vae.load_state_dict(self.checkpoint_vae['state_dict'])\n",
    "                else:\n",
    "                    self.model_vae.load_state_dict(self.checkpoint_vae)\n",
    "                    \n",
    "                self.model_vae.eval()\n",
    "                if self.fp16:\n",
    "                    self.model_vae.half()\n",
    "            except FileNotFoundError:\n",
    "                raise Exception(f\"Error: weight/{vae}.pth file not found.\")\n",
    "        else:\n",
    "            try:\n",
    "                from genconvit_ed import GenConViTED\n",
    "                from genconvit_vae import GenConViTVAE\n",
    "                self.model_ed = GenConViTED(config)\n",
    "                self.model_vae = GenConViTVAE(config)\n",
    "                self.checkpoint_ed = torch.load(f'weight/{ed}.pth', map_location=torch.device('cpu'))\n",
    "                self.checkpoint_vae = torch.load(f'weight/{vae}.pth', map_location=torch.device('cpu'))\n",
    "                if 'state_dict' in self.checkpoint_ed:\n",
    "                    self.model_ed.load_state_dict(self.checkpoint_ed['state_dict'])\n",
    "                else:\n",
    "                    self.model_ed.load_state_dict(self.checkpoint_ed)\n",
    "                if 'state_dict' in self.checkpoint_vae:\n",
    "                    self.model_vae.load_state_dict(self.checkpoint_vae['state_dict'])\n",
    "                else:\n",
    "                    self.model_vae.load_state_dict(self.checkpoint_vae)\n",
    "                self.model_ed.eval()\n",
    "                self.model_vae.eval()\n",
    "                if self.fp16:\n",
    "                    self.model_ed.half()\n",
    "                    self.model_vae.half()\n",
    "            except FileNotFoundError as e:\n",
    "                raise Exception(f\"Error: Model weights file not found.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.net == 'ed' :\n",
    "            x = self.model_ed(x)\n",
    "        elif self.net == 'vae':\n",
    "            x,_ = self.model_vae(x)\n",
    "        else:\n",
    "            x1 = self.model_ed(x)\n",
    "            x2,_ = self.model_vae(x)\n",
    "            x =  torch.cat((x1, x2), dim=0) #(x1+x2)/2 #\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Combined Model ---\n",
    "\n",
    "IMG_SIZE = 128\n",
    "FRAMES_PER_VIDEO = 5\n",
    "\n",
    "def create_combined_model(genconvit_config, genconvit_ed_weight, genconvit_vae_weight, genconvit_net,genconvit_fp16 ,input_shape=(FRAMES_PER_VIDEO, 224, 224, 3)):\n",
    "    \n",
    "    \n",
    "    genconvit_model = load_genconvit(genconvit_config, genconvit_net, genconvit_ed_weight, genconvit_vae_weight, genconvit_fp16)\n",
    "    base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False, weights ='imagenet',input_shape=(IMG_SIZE,IMG_SIZE, 3))\n",
    "\n",
    "    base_model.trainable = True\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # GenConViT Feature Extraction \n",
    "    def genconvit_feature_extractor(x):\n",
    "          \n",
    "        x_np = x.numpy()\n",
    "        #print(\"shape of input\",x_np.shape) # (FRAMES_PER_VIDEO,224,224,3)\n",
    "        x_tensor = preprocess_frame(x_np)\n",
    "        with torch.no_grad():\n",
    "            features = genconvit_model(x_tensor)\n",
    "        #print(\"shape of tensor\",features.shape)\n",
    "        return features.cpu().numpy()\n",
    "    \n",
    "    genconvit_layer = tf.keras.layers.Lambda(genconvit_feature_extractor)\n",
    "\n",
    "    genconvit_output = tf.keras.layers.TimeDistributed(genconvit_layer)(inputs) #(None,FRAMES_PER_VIDEO,2,768)\n",
    "\n",
    "    # Reshape tensor \n",
    "    if genconvit_net == 'both':\n",
    "        print(\"The shape in if condition\",genconvit_output.shape)\n",
    "        reshape_layer = tf.keras.layers.Reshape((FRAMES_PER_VIDEO,2,768,1))\n",
    "\n",
    "        reshaped_output = reshape_layer(genconvit_output)\n",
    "    else: \n",
    "         print(\"The shape in else condition\",genconvit_output.shape)\n",
    "         reshape_layer = tf.keras.layers.Reshape((FRAMES_PER_VIDEO,1,768,1))\n",
    "         reshaped_output = reshape_layer(genconvit_output)\n",
    "    # CNN (EfficientNet) for Spatial Feature Extraction\n",
    "    cnn_output= tf.keras.layers.TimeDistributed(base_model)(reshaped_output) # Apply base model to each frame's GenConViT output.\n",
    "\n",
    "    # LSTM for Temporal Modeling\n",
    "    x = tf.keras.layers.GlobalAveragePooling3D()(cnn_output)\n",
    "    x = tf.keras.layers.Reshape((1,-1))(x)\n",
    "    x = tf.keras.layers.LSTM(256)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    combined_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return combined_model\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "def load_and_preprocess_videos(path, label):\n",
    "    videos = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n",
    "            try:\n",
    "                videcap = cv2.VideoCapture(os.path.join(path, filename))\n",
    "                frames = []\n",
    "                success, image = videcap.read()\n",
    "                count = 0\n",
    "                while success and count < FRAMES_PER_VIDEO:\n",
    "                    image = cv2.resize(image, (224, 224))\n",
    "                    frames.append(image)\n",
    "                    success, image = videcap.read()\n",
    "                    count += 1\n",
    "                videcap.release()\n",
    "\n",
    "                if len(frames) == FRAMES_PER_VIDEO:\n",
    "                    videos.append(np.array(frames))\n",
    "                    labels.append(label)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {filename}: {type(e).__name__} - {e}\")\n",
    "\n",
    "    return np.array(videos), np.array(labels)\n",
    "\n",
    "# --- Training and Evaluation ---\n",
    "\n",
    "# Main execution block (for notebook)\n",
    "if __name__ == '__main__':\n",
    "    # Define paths and model parameters\n",
    "    real_videos_path = r\"C:\\Users\\rajku\\Downloads\\archive\\Celeb-real\"\n",
    "    fake_videos_path = r\"C:\\Users\\rajku\\Downloads\\archive\\Celeb-synthesis2500\"\n",
    "    \n",
    "\n",
    "    genconvit_config = {\n",
    "        'image_size': 224,\n",
    "        'patch_size': 16,\n",
    "        'num_classes': 2,\n",
    "        'dim': 768,\n",
    "        'depth': 12,\n",
    "        'heads': 12,\n",
    "        'mlp_dim': 3072,\n",
    "        'dropout': 0.1,\n",
    "        'emb_dropout': 0.1,\n",
    "    }\n",
    "\n",
    "    genconvit_ed_weight = 'ed_best'\n",
    "    genconvit_vae_weight = 'vae_best'\n",
    "    genconvit_net = 'both' # 'ed', 'vae', 'both'\n",
    "    genconvit_fp16 = False\n",
    "\n",
    "    # Load and preprocess data\n",
    "    real_videos, real_labels = load_and_preprocess_videos(real_videos_path, 0)\n",
    "    fake_videos, fake_labels = load_and_preprocess_videos(fake_videos_path, 1)\n",
    "\n",
    "    # Combine real and fake data\n",
    "    X = np.concatenate([real_videos, fake_videos])\n",
    "    Y = np.concatenate([real_labels, fake_labels])\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create the combined model\n",
    "    combined_model = create_combined_model(genconvit_config, genconvit_ed_weight, genconvit_vae_weight, genconvit_net, genconvit_fp16 )\n",
    "\n",
    "    # Compile and train the model\n",
    "    combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = combined_model.fit(\n",
    "        X_train, Y_train,\n",
    "        epochs=2, batch_size=8,  # Adjust based on your resources\n",
    "        validation_data=(X_test, Y_test)\n",
    "    )\n",
    "\n",
    "    # Evaluation and Visualization\n",
    "    _, accuracy = combined_model.evaluate(X_test, Y_test)\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "    # Plotting the training history\n",
    "    def plot_history(history):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    plot_history(history)\n",
    "\n",
    "\n",
    "    # Prediction on new video\n",
    "    def predict_video(video_path):\n",
    "        try:\n",
    "            vidcap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            success, image = vidcap.read()\n",
    "            count = 0\n",
    "            while success and count < FRAMES_PER_VIDEO:\n",
    "                image = cv2.resize(image, (224, 224))\n",
    "                frames.append(image)\n",
    "                success, image = vidcap.read()\n",
    "                count += 1\n",
    "            vidcap.release()\n",
    "            if len(frames) == FRAMES_PER_VIDEO:\n",
    "                video = np.expand_dims(np.array(frames), axis=0)\n",
    "                prediction = combined_model.predict(video)[0][0]\n",
    "                return prediction\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video: {e}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    # Example Prediction\n",
    "    prediction = predict_video(r\"archive\\Celeb-synthesis\\id61_id60_0009.mp4\")\n",
    "     # replace with test video path\n",
    "    if prediction is not None:\n",
    "        if prediction > 0.5:\n",
    "            print(\"Prediction: Fake (Probability:\", prediction, \")\")\n",
    "        else:\n",
    "            print(\"Prediction: Real (Probability:\", prediction, \")\")\n",
    "    else:\n",
    "      print(\"Error processing video.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
